load_module modules/ngx_http_js_module.so;

worker_processes auto;
error_log /var/log/nginx/error.log warn;

events {
    worker_connections 1024;
}

http {
    js_import auth from scripts/auth.js;
    js_import stats from scripts/stats.js;

    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent"';
    access_log /var/log/nginx/access.log main;

    # Upstream: LMGate Python service
    upstream lmgate {
        server lmgate:8081;
    }

    # Upstream: mock LLM provider (replaces real APIs for e2e testing)
    upstream openai {
        server mock-upstream:8082;
    }


    server {
        listen 80;

        # Auth subrequest endpoint (internal)
        location = /_auth {
            internal;
            proxy_pass http://lmgate/auth;
            proxy_pass_request_body off;
            proxy_set_header Content-Length "";
            proxy_set_header X-Original-URI $request_uri;
            proxy_set_header Authorization $http_authorization;
            proxy_set_header X-Api-Key $http_x_api_key;
        }

        # Stats endpoint (internal, used by njs)
        location = /_stats {
            internal;
            proxy_pass http://lmgate/stats;
        }

        # OpenAI provider (routed to mock-upstream in e2e)
        location /openai/ {
            auth_request /_auth;
            auth_request_set $lmgate_id $upstream_http_x_lmgate_id;
            set $upstream_host api.openai.com;

            proxy_pass http://openai/;
            proxy_set_header Host mock-upstream;

            proxy_buffering off;

            js_body_filter stats.accumulate;
        }

        # Health check
        location /healthz {
            proxy_pass http://lmgate/healthz;
        }
    }
}
